# 검색바 프로젝트 수정하기 위한 크롤링

## 검색바를 가져올 데이터를 고르자

앞에 만든 프로젝트는 해당 암 이름이 적혀진 데이터를 주어져서 만든 내용이라서 해당 서버 내용을 포함하고 있지 않아 새로운 데이터를 찾아 보았습니다.

처음에는 [공공데이터](https://www.data.go.kr/)중 수질 관련 내용을 가져올까 했는데.. 데이터가 최신이 아니고. 이상하게 적합인데 수치도 제대로 나타나지 않고 해당 년도에 부적합 내용이 뉴스에 존재하는데 해당 데이터에는 다 적합이라고 되어 있어서 그냥 다른 걸 찾기로 하였습니다.

다음 정보를 구한 곳은 [kaggle](https://www.kaggle.com/)로 인공지능 학습을 위한 데이터를 넣어두는 곳으로 여러 개 찾아보다 포켓몬에 대한 정보를 보게 되어 오랜만에 생각이 나서 해당 데이터를 사용하기로 결정 하였습니다.

### 데이터 자료 찾은 위치

- [포켓몬DB](https://pokemondb.net/)로 포켓몬에 대한 정보 및 이미지가 있어서 선택하였습니다.

## 이미지를 가져오기 위한 크롤링 사용

- 포켓몬에 기본 정보 경우 해당 [포켓몬 정보 텍스트 리스트](https://pokemondb.net/tools/text-list)로 CSV형식으로 가져 올수 있었으며 해당 파일을 나중에 JSON 형식으로 변경할 생각입니다.
- 이미지 경우 필요한 이미지를 가져올려면 해당 이미지를 일일히 저장하거나 주어진 경로로 가져오는 방식을 사용해야 해서 1000개가 넘은 이미지를 일일히 저장하기에는 너무 힘들어 **크롤링**을 생각하였습니다.

### python 라이브러리 설치

- **BeautifulSoup** 설치
  - 해당 라이브러리는 html과 xml을 분석하고 파싱해주는 라이브러리 이다.

    ```bash
    pip3 install beautifulsoup4
    ```

- pip3를 하기전 설치를 하는데 에러가 나서 확인해보니 업그레이드가 안 되어 있던 경우여서 업데이트를 해주었습니다.

  ```bash
  sudo apt update -y && sudo apt upgrade -y
  sudo apt install python3-pip
  ```

- 패키지 및 라이브러리 설정

  ```python
  from bs4 import BeautifulSoup
  import requests
  from urllib.request import urlretrieve
  ```

### 크롤링 (코드)  

- 크롤링을 하고자하면 Home URL뒤에 **`/robots.txt`** 을 하면 크롤링이 가능한지 얼마나 딜레이를 하는지 등을 적어놓은 내용이 존재합니다.
- 찾고자 하는 내용 상위 태그 가져오기

  ```python
  URL = "갖고자 하는 데이터가 있는 링크"
  html = requests.get(URL)
  soup = BeautifulSoup(html.text, 'html.parser')
  html.close()
  # 갖고자 하는 태그와 그걸 판단 할수 있는 텍스트
  datal_list = soup.findAll('span',{'class':'판단할 수 있을 class 내임'})
  ```

- datal_list에 속에 있는 내가 찾고자 하는 태그들
- 저는 찾고자 하는 내용이 **picture** 또는 **a** 태그 안에 있기 떄문에 for문을 돌면서 해당 태그전체를 찾기 위해 findAll을 사용하였습니다.

  ```python
  param_list = []
  for data in datal_list:
      picture = data.findAll('picture');
      param_list.extend(picture if picture != [] else data.findAll('a'))
  ```

- 찾고자하는 **img** 정보와 이름 정보 및 **source** 태그
- 제가 찾는 이미지가 **source**에만 있는 줄 알았으나 일부 **img** 태그에만 존재한다를 걸 알고 추가로 조건을 적어 주었습니다.
- 그냥 **if sorceData**로 하여서 할 경우 **NoDeType**을 찾을 수 없다는 에러가 발생하거나 else 부분으로 img 태그에 있는 부분만 찾아서 조건을 **sourceData.attrs**를 주어 **srcset**이 존재 할 경우 path 값을 가지도록 수정하였습니다.

  ```python
  for item in param_list:
    sourceData = item.find('source');
    altData = item.find('img');
    if sourceData and 'srcset' in sourceData.attrs:
        path = sourceData['srcset']
    else:
        path = altData['src']
    types = path.split('.')
    types = types[len(types) - 1]
    name = altData['alt'].replace(' ', '-')
  ```

#### 403 에러코드로 인한 접근 금지

- 'urlretrieve'를 사용하여 해당 경로의 이미지를 저장할려고 하였는데 403에러가 발생하였습니다.

  ```python
    urlretrieve(path, '{name}.types')
  ```

- 해당 에러를 해결하기위해 찾아보았는데 `User-Agent`를 이용해 데이터를 가져오는 서버를 속여서 가져와야 한다고 하여 몇가지 수정하였습니다.
- header에 `User-Agent`를 설정 하였고 `urlretrieve`는 headers를 설정 못하기 때문에 Request, urlopen으로 변경 하였습니다.
- **tiem.sleep(5)** 를 주어 robots.txt 해당 서버에 부하를 줄이고 크롤링이 막히는 것을 방지하였습니다.

  ```python
  import requests, os, time
  from urllib.request import Request, urlopen

  headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}

  for item in param_list:
    sourceData = item.find('source');
    altData = item.find('img');
    if sourceData and 'srcset' in sourceData.attrs:
        path = sourceData['srcset']
    else:
        path = altData['src']
    types = path.split('.')
    types = types[len(types) - 1]
    name = altData['alt'].replace(' ', '-')
    req = Request(url=path, headers=headers)
    response = urlopen(req)
    data = response.read()
    with open(f'./poketsprite/{name}.{types}','wb') as file:
        file.write(data)
    time.sleep(5)
  ```
